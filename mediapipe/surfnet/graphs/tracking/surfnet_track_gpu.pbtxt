# Surfnet track for desktop using GPU
# Still uses the mobile calculators defined in BUILD
# requires run_graph_main_gpu to be built

profiler_config {
  trace_enabled: true
  enable_profiler: true
  trace_log_interval_count: 50
}


# Images on GPU coming into and out of the graph.
input_stream: "input_video"
output_stream: "output_video"

# Resamples the images by specific frame rate. This calculator is used to
# control the frequecy of subsequent calculators/subgraphs, e.g. less power
# consumption for expensive process.
# node {
#   calculator: "PacketResamplerCalculator"
#   input_stream: "DATA:input_video"
#   output_stream: "DATA:throttled_input_video"
#   node_options: {
#     [type.googleapis.com/mediapipe.PacketResamplerCalculatorOptions] {
#       frame_rate: 1.0
#     }
#   }
# }

node {
  calculator: "FlowLimiterCalculator"
  input_stream: "input_video"
  input_stream: "FINISHED:detections"
  input_stream_info: {
    tag_index: "FINISHED"
    back_edge: true
  }
  output_stream: "throttled_input_video"
}

# Transforms the input image on 640x640 image.
# node {
#   calculator: "ImageTransformationCalculator"
#   input_stream: "IMAGE_GPU:throttled_input_video"
#   output_stream: "IMAGE_GPU:scaled_video"
#   node_options: {
#     [type.googleapis.com/mediapipe.ImageTransformationCalculatorOptions] {
#       output_width: 640
#       output_height: 640
#     }
#   }
# }

# Transforms the input image on 640x640 image.
node {
  calculator: "ImageCroppingCalculator"
  input_stream: "IMAGE_GPU:throttled_input_video"
  output_stream: "IMAGE_GPU:cropped_video"
  node_options: {
    [type.googleapis.com/mediapipe.ImageCroppingCalculatorOptions] {
      norm_center_x: 0.5
      norm_center_y: 0.5
      width: 1080
      height: 1080
    }
  }
}


# Transforms the input image on 640x640 image.
node {
  calculator: "ImageTransformationCalculator"
  input_stream: "IMAGE_GPU:cropped_video"
  output_stream: "IMAGE_GPU:scaled_video"
  node_options: {
    [type.googleapis.com/mediapipe.ImageTransformationCalculatorOptions] {
      output_width: 640
      output_height: 640
      flip_vertically: false
      flip_horizontally: false
    }
  }
}


node {
  calculator: "TfLiteConverterCalculator"
  input_stream: "IMAGE_GPU:scaled_video"
  output_stream: "TENSORS_GPU:image_tensor"
  node_options: {
      [type.googleapis.com/mediapipe.TfLiteConverterCalculatorOptions] {
      zero_center: false
    }
  }
}

node {
  calculator: "TfLiteInferenceCalculator"
  input_stream: "TENSORS_GPU:image_tensor"
  output_stream: "TENSORS:detection_tensors"
  node_options: {
    [type.googleapis.com/mediapipe.TfLiteInferenceCalculatorOptions] {
      # model_path: "mediapipe/surfnet/models/yolo_pau-fp16.tflite"
      model_path: "mediapipe/surfnet/models/yolo_surfnet-fp16.tflite"
      # model_path: "mediapipe/surfnet/models/yolov5s-fp16-optimize.tflite"
      delegate: { 
        gpu { 
          use_advanced_gpu_api: true
          allow_precision_loss: true
        }
      }
    }
  }
}

node {
  calculator: "TfLiteYoloTensorsToDetectionsCalculator"
  input_stream: "TENSORS:detection_tensors"
  output_stream: "DETECTIONS:detections"
  node_options: {
    [type.googleapis.com/mediapipe.TfLiteYoloTensorsToDetectionsCalculatorOptions] {
      num_coords: 17
      # 85 for standard yolo = num classes + 5 
      max_detections: 100
      num_boxes: 25200
      min_conf_thresh: 0.35
      flip_vertically: false
      reverse_output_order: true
      # ignore_classes: [9]
    }
  }
}

# Performs non-max suppression to remove excessive detections.
node {
  calculator: "NonMaxSuppressionCalculator"
  input_stream: "detections"
  output_stream: "filtered_detections"
  node_options: {
    [type.googleapis.com/mediapipe.NonMaxSuppressionCalculatorOptions] {
      min_suppression_threshold: 0.4
      max_num_detections: 6
      overlap_type: INTERSECTION_OVER_UNION
      return_empty_detections: true
    }
  }
}

# Maps detection label IDs to the corresponding label text. The label map is
# provided in the label_map_path option.
node {
  calculator: "DetectionLabelIdToTextCalculator"
  input_stream: "filtered_detections"
  output_stream: "named_detections"
  node_options: {
    [type.googleapis.com/mediapipe.DetectionLabelIdToTextCalculatorOptions] {
      #label_map_path: "mediapipe/surfnet/models/labelmap_yolo.txt"
      label_map_path: "mediapipe/surfnet/models/labelmap_surfnet.txt"
    }
  }
}

node {
  calculator: "DetectionUniqueIdCalculator"
  input_stream: "DETECTIONS:named_detections"
  output_stream: "DETECTIONS:detections_with_id"
}

# Converts detections to TimedBox protos which are used as initial location
# for tracking.
node {
  calculator: "DetectionsToTimedBoxListCalculator"
  input_stream: "DETECTIONS:detections_with_id"
  output_stream: "BOXES:start_pos"
}

node: {
  calculator: "ImageTransformationCalculator"
  input_stream: "IMAGE_GPU:cropped_video"
  output_stream: "IMAGE_GPU:downscaled_input_video"
  node_options: {
    [type.googleapis.com/mediapipe.ImageTransformationCalculatorOptions] {
      output_width: 640
      output_height: 640
    }
  }
}

# Converts GPU buffer to ImageFrame for processing tracking.
node: {
  calculator: "GpuBufferToImageFrameCalculator"
  input_stream: "downscaled_input_video"
  output_stream: "downscaled_input_video_cpu"
}

####
#### Try to visualize the flow (not working)
####

# # Join the original input stream and the one that is shifted by one packet.
# node: {
#   calculator: "PacketInnerJoinCalculator"
#   input_stream: "downscaled_input_video_cpu"
#   input_stream: "prev_output"
#   output_stream: "first_frames"
#   output_stream: "second_frames"
# }

# # Compute the forward optical flow.
# node {
#   calculator: "Tvl1OpticalFlowCalculator"
#   input_stream: "FIRST_FRAME:first_frames"
#   input_stream: "SECOND_FRAME:second_frames"
#   output_stream: "FORWARD_FLOW:forward_flow"
#   max_in_flight: 10
# }

# # Convert an optical flow to be an image frame with 2 channels (v_x and v_y),
# # each channel is quantized to 0-255.
# node: {
#   calculator: "FlowToImageCalculator"
#   input_stream: "forward_flow"
#   output_stream: "flow_frames"
#   node_options: {
#     [type.googleapis.com/mediapipe.FlowToImageCalculatorOptions]: {
#       min_value: -20.0
#       max_value: 20.0
#     }
#   }
# }
# 
# node: {
#   calculator: "ImageFrameToGpuBufferCalculator"
#   input_stream: "flow_frames"
#   output_stream: "output_video"
# }

#### 

# Performs motion analysis on an incoming video stream.
node: {
  calculator: "MotionAnalysisCalculator"
  input_stream: "VIDEO:downscaled_input_video_cpu"
  output_stream: "CAMERA:camera_motion"
  output_stream: "FLOW:region_flow"

  node_options: {
    [type.googleapis.com/mediapipe.MotionAnalysisCalculatorOptions]: {
      analysis_options {
        analysis_policy: ANALYSIS_POLICY_CAMERA_MOBILE
        flow_options {
          fast_estimation_min_block_size: 100
          top_inlier_sets: 1
          frac_inlier_error_threshold: 3e-3
          downsample_mode: DOWNSAMPLE_TO_INPUT_SIZE
          verification_distance: 5.0
          verify_long_feature_acceleration: true
          verify_long_feature_trigger_ratio: 0.1
          tracking_options {
            max_features: 500
            adaptive_extraction_levels: 2
            min_eig_val_settings {
              adaptive_lowest_quality_level: 2e-4
            }
            klt_tracker_implementation: KLT_OPENCV
          }
        }
      }
    }
  }
}


# Packages camera motion and flow in region data. Does not seem to work well
node: {
  calculator: "FlowPackagerCalculator"
  input_stream: "FLOW:region_flow"
  input_stream: "CAMERA:camera_motion"
  output_stream: "TRACKING:tracking_data"

  node_options: {
    [type.googleapis.com/mediapipe.FlowPackagerCalculatorOptions]: {
      flow_packager_options: {
        binary_tracking_data_support: false
      }
    }
  }
}


# # Tracks box positions over time.
node: {
  calculator: "BoxTrackerCalculator"
  input_stream: "TRACKING:tracking_data"
  # input_stream: "VIDEO:downscaled_input_video_cpu"
  input_stream: "TRACK_TIME:input_video"
  input_stream: "START_POS:start_pos"
  input_stream: "CANCEL_OBJECT_ID:cancel_object_id"
  input_stream_info: {
    tag_index: "CANCEL_OBJECT_ID"
    back_edge: true
  }
  output_stream: "BOXES:boxes"
  # output_stream: "VIZ:cpu_output_viz"

  input_stream_handler {
    input_stream_handler: "SyncSetInputStreamHandler"
    options {
      [mediapipe.SyncSetInputStreamHandlerOptions.ext] {
        sync_set {
          tag_index: "TRACKING"
          tag_index: "TRACK_TIME"
        }
        sync_set {
          tag_index: "START_POS"
        }
        sync_set {
          tag_index: "CANCEL_OBJECT_ID"
        }
      }
    }
  }

  node_options: {
    [type.googleapis.com/mediapipe.BoxTrackerCalculatorOptions]: {
      tracker_options: {
        track_step_options {
          track_object_and_camera: true
          tracking_degrees: TRACKING_DEGREE_OBJECT_SCALE
          inlier_spring_force: 0.0
          static_motion_temporal_ratio: 3e-2
        }
      }
      visualize_tracking_data: false
      # visualize_state: true
      # visualize_internal_state: true

      streaming_track_data_cache_size: 100
    }
  }
}


# # Managers new detected objects and objects that are being tracked.
# # It associates the duplicated detections and updates the locations of
# # detections from tracking.
node: {
  calculator: "TrackedDetectionManagerCalculator"
  input_stream: "DETECTIONS:detections_with_id"
  input_stream: "TRACKING_BOXES:boxes"
  output_stream: "DETECTIONS:tracked_detections"
  output_stream: "CANCEL_OBJECT_ID:cancel_object_id"

  input_stream_handler {
    input_stream_handler: "SyncSetInputStreamHandler"
    options {
      [mediapipe.SyncSetInputStreamHandlerOptions.ext] {
        sync_set {
          tag_index: "TRACKING_BOXES"
        }
        sync_set {
          tag_index: "DETECTIONS"
        }
      }
    }
  }
}

# Converts the detections to drawing primitives for annotation overlay.
node {
  calculator: "DetectionsToRenderDataCalculator"
  input_stream: "DETECTIONS:tracked_detections"
  output_stream: "RENDER_DATA:render_data"
  node_options: {
    [type.googleapis.com/mediapipe.DetectionsToRenderDataCalculatorOptions] {
      thickness: 4.0
      color { r: 255 g: 0 b: 0 }
      render_detection_id: true
    }
  }
}

# Draws annotations and overlays them on top of the input images.
node {
  calculator: "AnnotationOverlayCalculator"
  input_stream: "IMAGE_GPU:cropped_video"
  input_stream: "render_data"
  output_stream: "IMAGE_GPU:output_video"
}
